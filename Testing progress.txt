Run 12 - 0.84 Max Dice on Val-020:
	xform = Compose([
			LoadNiftid(keys),
			AddChanneld(keys),
			Spacingd(keys, 0.5, diagonal=True, mode=('bilinear', 'nearest')),
			Orientationd(keys, axcodes='RAS'),
			ScaleIntensityd("image"),
			CropForegroundd(keys, source_key="image"),
			# RandAffined(keys, mode=('bilinear', 'nearest'), rotate_range=(0.1,0.1,0.1), scale_range=(0.05,0.05,0.05), prob=0.05, device=device),
			# RandSpatialCropSamplesd(keys, (96,96,96), 7, random_size=False),
			RandCropByPosNegLabeld(keys, label_key='label', spatial_size=(96,96,96), pos=0.8, neg=0.2, num_samples=7),
			ToTensord(keys)
		])
	net = UNet(dimensions=3, in_channels=1, out_channels=1, channels=(16, 32, 64, 128, 256),
               strides=(2, 2, 2, 2), num_res_units=2, norm=Norm.BATCH, dropout=0.05).to(device)
	loss = GeneralizedDiceLoss(sigmoid=True)
	opt = torch.optim.Adam(net.parameters(), 1e-3)
	
	Notes: Dataset not cleaned, may have poor training examples
			19/36 Training examples with issues
			Dice score only after manually cleaning seg, should discard this entry after re-running training
			
			
Run 13 - Val mean dice 0.83 at epoch 1390 (0.81 at epoch 1500):
	xform = Compose([
			LoadNiftid(keys),
			AddChanneld(keys),
			Spacingd(keys, 0.5, diagonal=True, mode=('bilinear', 'nearest')),
			Orientationd(keys, axcodes='RAS'),
			ScaleIntensityd("image"),
			CropForegroundd(keys, source_key="image"),
			# RandAffined(keys, mode=('bilinear', 'nearest'), rotate_range=(0.1,0.1,0.1), scale_range=(0.05,0.05,0.05), prob=0.05, device=device),
			# RandSpatialCropSamplesd(keys, (96,96,96), 7, random_size=False),
			RandCropByPosNegLabeld(keys, label_key='label', spatial_size=(96,96,96), pos=0.8, neg=0.2, num_samples=7),
			ToTensord(keys)
		])
	net = UNet(dimensions=3, in_channels=1, out_channels=1, channels=(16, 32, 64, 128, 256),
				   strides=(2, 2, 2, 2), num_res_units=2, norm=Norm.BATCH, dropout=0.05).to(device)
	loss = GeneralizedDiceLoss(sigmoid=True)
	opt = torch.optim.Adam(net.parameters(), 1e-3)

	Notes: 
	
	
	
Run 14 - Val mean dice 0.82 at epoch 890 (0.81 at 1500)
	xform = Compose([
        LoadNiftid(keys),
        AddChanneld(keys),
        Spacingd(keys, 0.5, diagonal=True, mode=('bilinear', 'nearest')),
        Orientationd(keys, axcodes='RAS'),
        ScaleIntensityd("image"),
        CropForegroundd(keys, source_key="image"),
        # RandAffined(keys, mode=('bilinear', 'nearest'), rotate_range=(0.1,0.1,0.1), scale_range=(0.05,0.05,0.05), prob=0.05, device=device),
        RandCropByPosNegLabeld(keys, label_key='label', spatial_size=(96,96,96), pos=0.8, neg=0.2, num_samples=7),
        ToTensord(keys)
    ])
	net = UNet(dimensions=3, in_channels=1, out_channels=1, channels=(16, 32, 64, 128, 256),
               strides=(2, 2, 2, 2), num_res_units=2, norm=Norm.BATCH, dropout=0.25).to(device)
    loss = GeneralizedDiceLoss(sigmoid=True)
    opt = torch.optim.Adam(net.parameters(), 1e-3)
	
	Notes: Increased dropout led to worse training error, however little to no change in validation performance (batch norm may make dropout unecessary)
	
	
Run 15 - Val mean dice 0.825 at epoch 830 (0.807 at 1500)
	xform = Compose([
        LoadNiftid(keys),
        AddChanneld(keys),
        Spacingd(keys, 0.5, diagonal=True, mode=('bilinear', 'nearest')),
        Orientationd(keys, axcodes='RAS'),
        ScaleIntensityd("image"),
        CropForegroundd(keys, source_key="image"),
        # RandAffined(keys, mode=('bilinear', 'nearest'), rotate_range=(0.1,0.1,0.1), scale_range=(0.05,0.05,0.05), prob=0.05, device=device),
        RandCropByPosNegLabeld(keys, label_key='label', spatial_size=(96,96,96), pos=0.8, neg=0.2, num_samples=7),
        ToTensord(keys)
    ])
	net = UNet(dimensions=3, in_channels=1, out_channels=1, channels=(16, 32, 64, 128, 256),
               strides=(2, 2, 2, 2), num_res_units=2, norm=Norm.BATCH, dropout=0).to(device)
    loss = GeneralizedDiceLoss(sigmoid=True)
    opt = torch.optim.Adam(net.parameters(), 1e-3)
	
	Notes: No dropout results in approx same validation error, better training error. Some disconnected pieces appear.
	
	
Run 18 - Val mean dice 0.827 at epoch 1190 (0.813 at 1500)
	xform = Compose([
        LoadNiftid(keys),
        AddChanneld(keys),
        Spacingd(keys, 0.5, diagonal=True, mode=('bilinear', 'nearest')),
        Orientationd(keys, axcodes='RAS'),
        ScaleIntensityd("image"),
        CropForegroundd(keys, source_key="image"),
        RandAffined(keys, mode=('bilinear', 'nearest'), rotate_range=(0.1,0.1,0.1), scale_range=(0.05,0.05,0.05), prob=0.2, as_tensor_output=False, device=device),
        RandCropByPosNegLabeld(keys, label_key='label', spatial_size=(96,96,96), pos=0.8, neg=0.2, num_samples=7),
        ToTensord(keys)
    ])
	net = UNet(dimensions=3, in_channels=1, out_channels=1, channels=(16, 32, 64, 128, 256),
               strides=(2, 2, 2, 2), num_res_units=2, norm=Norm.BATCH, dropout=0).to(device)
    loss = GeneralizedDiceLoss(sigmoid=True)
    opt = torch.optim.Adam(net.parameters(), 1e-3)
	
	Notes: Spatial transformation has little impact. Possibly helps with leaflet length. Some disconnected pieces appear.
			After removing disconnected componenents, we see improvement in Dice and Hausdorff over Run 13 and Run 15 on 022 and 058, but Run 13 is better on 033 and 038.
	

Run 19 - Val mean dice 0.827 at epoch 660 (0.788 at 1500)
	xform = Compose([
        LoadNiftid(keys),
        AddChanneld(keys),
        Spacingd(keys, 0.5, diagonal=True, mode=('bilinear', 'nearest')),
        Orientationd(keys, axcodes='RAS'),
        ScaleIntensityd("image"),
        CropForegroundd(keys, source_key="image"),
        #RandAffined(keys, mode=('bilinear', 'nearest'), rotate_range=(0.15,0.15,0.15), scale_range=(0.05,0.05,0.05), prob=0.2, as_tensor_output=False, device=device),
        Rand3DElasticd(keys, mode=('bilinear', 'nearest'),  rotate_range=(0.15,0.15,0.15), scale_range=(0.05,0.05,0.05),
                       sigma_range=(0,1), magnitude_range=(0,2), prob=0.2, as_tensor_output=False, device=device),
        RandCropByPosNegLabeld(keys, label_key='label', spatial_size=(96,96,96), pos=0.8, neg=0.2, num_samples=7),
        ToTensord(keys)
    ])
	net = UNet(dimensions=3, in_channels=1, out_channels=1, channels=(16, 32, 64, 128, 256),
               strides=(2, 2, 2, 2), num_res_units=2, norm=Norm.BATCH, dropout=0).to(device)
    loss = GeneralizedDiceLoss(sigmoid=True)
    opt = torch.optim.Adam(net.parameters(), 1e-3)
	
	Notes: Testing elastic deformation.


Run 21 - Val mean dice 0.814 at epoch 120 (0.798 at 1500)
	xform = Compose([
			LoadNiftid(keys),
			AddChanneld(keys),
			Spacingd(keys, 0.5, diagonal=True, mode=('bilinear', 'nearest')),
			Orientationd(keys, axcodes='RAS'),
			ScaleIntensityd("image"),
			CropForegroundd(keys, source_key="image"),
			RandCropByPosNegLabeld(keys, label_key='label', spatial_size=(96,96,96), pos=0.8, neg=0.2, num_samples=7),
			ToTensord(keys)
		])
	net = UNet(dimensions=3, in_channels=1, out_channels=1, channels=(16, 32, 64, 128, 256),
               strides=(2, 2, 2, 2), num_res_units=2, norm=Norm.BATCH, dropout=0).to(device)
    loss = GeneralizedDiceLoss(sigmoid=True)
    opt = torch.optim.Adam(net.parameters(), 1e-3)
	
	lr_scheduler = StepLR(opt, 400)
	
	Notes: Testing no LR scheduling with 400 epoch step
	

Run 22 - Val mean dice 0.813 at epoch 620 (0.811 at 800)
	xform = Compose([
			LoadNiftid(keys),
			AddChanneld(keys),
			Spacingd(keys, 0.5, diagonal=True, mode=('bilinear', 'nearest')),
			Orientationd(keys, axcodes='RAS'),
			ScaleIntensityd("image"),
			CropForegroundd(keys, source_key="image"),
			RandCropByPosNegLabeld(keys, label_key='label', spatial_size=(96,96,96), pos=0.8, neg=0.2, num_samples=7),
			ToTensord(keys)
		])
	net = UNet(dimensions=3, in_channels=1, out_channels=1, channels=(16, 32, 64, 128, 256),
               strides=(2, 2, 2, 2), num_res_units=2, norm=Norm.BATCH, dropout=0.1).to(device)
    loss = GeneralizedDiceLoss(sigmoid=True)
    opt = torch.optim.Adam(net.parameters(), 1e-2)

    # trainer = create_supervised_trainer(net, opt, loss, device, False, )
    trainer = SupervisedTrainer(
        device=device,
        max_epochs=800,
        train_data_loader=loader,
        network=net,
        optimizer=opt,
        loss_function=loss,
        key_train_metric={"train_meandice": MeanDice(sigmoid=True,output_transform=lambda x: (x["pred"], x["label"]))},
    )
	
	lr_scheduler = StepLR(opt, 200)

	
Run 23 - Val mean dice 0.807 at epoch 330 (0.797 at 800)
	xform = Compose([
			LoadNiftid(keys),
			AddChanneld(keys),
			Spacingd(keys, 0.5, diagonal=True, mode=('bilinear', 'nearest')),
			Orientationd(keys, axcodes='RAS'),
			ScaleIntensityd("image"),
			CropForegroundd(keys, source_key="image"),
			RandCropByPosNegLabeld(keys, label_key='label', spatial_size=(96,96,96), pos=0.8, neg=0.2, num_samples=7),
			ToTensord(keys)
		])
	net = UNet(dimensions=3, in_channels=1, out_channels=1, channels=(16, 32, 64, 128, 256),
               strides=(2, 2, 2, 2), num_res_units=2, norm=Norm.BATCH, dropout=0).to(device)
    loss = GeneralizedDiceLoss(sigmoid=True)
    opt = torch.optim.Adam(net.parameters(), 1e-2)

    # trainer = create_supervised_trainer(net, opt, loss, device, False, )
    trainer = SupervisedTrainer(
        device=device,
        max_epochs=800,
        train_data_loader=loader,
        network=net,
        optimizer=opt,
        loss_function=loss,
        key_train_metric={"train_meandice": MeanDice(sigmoid=True,output_transform=lambda x: (x["pred"], x["label"]))},
    )
	
	lr_scheduler = StepLR(opt, 200)
	
	Note: Scheduler with no dropout plateaus lower than with dropout
	
	
Run 24 - Val mean dice 0.801 at epoch 560 (0.80 at 800)
	xform = Compose([
			LoadNiftid(keys),
			AddChanneld(keys),
			Spacingd(keys, 0.5, diagonal=True, mode=('bilinear', 'nearest')),
			Orientationd(keys, axcodes='RAS'),
			ScaleIntensityd("image"),
			CropForegroundd(keys, source_key="image"),
			RandCropByPosNegLabeld(keys, label_key='label', spatial_size=(96,96,96), pos=0.8, neg=0.2, num_samples=7),
			ToTensord(keys)
		])
	net = UNet(dimensions=3, in_channels=1, out_channels=1, channels=(16, 32, 64, 128, 256),
               strides=(2, 2, 2, 2), num_res_units=2, norm=Norm.BATCH, dropout=0.25).to(device)
    loss = GeneralizedDiceLoss(sigmoid=True)
    opt = torch.optim.Adam(net.parameters(), 1e-2)

    # trainer = create_supervised_trainer(net, opt, loss, device, False, )
    trainer = SupervisedTrainer(
        device=device,
        max_epochs=800,
        train_data_loader=loader,
        network=net,
        optimizer=opt,
        loss_function=loss,
        key_train_metric={"train_meandice": MeanDice(sigmoid=True,output_transform=lambda x: (x["pred"], x["label"]))},
    )
	
	lr_scheduler = StepLR(opt, 200)
	
	Note: Slightly worse performance than 0.1 dropout
	
Run 25 - Val mean dice 0.801 at epoch 250 (0.797 at 800)
	xform = Compose([
			LoadNiftid(keys),
			AddChanneld(keys),
			Spacingd(keys, 0.5, diagonal=True, mode=('bilinear', 'nearest')),
			Orientationd(keys, axcodes='RAS'),
			ScaleIntensityd("image"),
			CropForegroundd(keys, source_key="image"),
			RandCropByPosNegLabeld(keys, label_key='label', spatial_size=(96,96,96), pos=0.8, neg=0.2, num_samples=4),
			ToTensord(keys)
		])
	loader = DataLoader(ds, batch_size=8, shuffle=True, num_workers=0, drop_last=True)
	net = UNet(dimensions=3, in_channels=1, out_channels=1, channels=(16, 32, 64, 128, 256),
               strides=(2, 2, 2, 2), num_res_units=2, norm=Norm.BATCH, dropout=0).to(device)
    loss = GeneralizedDiceLoss(sigmoid=True)
    opt = torch.optim.Adam(net.parameters(), 1e-2)

    # trainer = create_supervised_trainer(net, opt, loss, device, False, )
    trainer = SupervisedTrainer(
        device=device,
        max_epochs=800,
        train_data_loader=loader,
        network=net,
        optimizer=opt,
        loss_function=loss,
        key_train_metric={"train_meandice": MeanDice(sigmoid=True,output_transform=lambda x: (x["pred"], x["label"]))},
    )
	
	lr_scheduler = StepLR(opt, 200)
	
	Note: Reduced number of samples of each volume to 4 (from 7) and increased batch size to 8 (from 4). Testing if increased batch size helps with normalization, volumes were possibly oversampled to start as well.
	
	
Run 26 - Val mean dice 0.816 at epoch 190 (0.804 at 1000)
	xform = Compose([
			LoadNiftid(keys),
			AddChanneld(keys),
			Spacingd(keys, 0.5, diagonal=True, mode=('bilinear', 'nearest')),
			Orientationd(keys, axcodes='RAS'),
			ScaleIntensityd("image"),
			CropForegroundd(keys, source_key="image"),
			RandCropByPosNegLabeld(keys, label_key='label', spatial_size=(96,96,96), pos=0.8, neg=0.2, num_samples=4),
			ToTensord(keys)
		])
	loader = DataLoader(ds, batch_size=8, shuffle=True, num_workers=0, drop_last=True)
    net = UNet(dimensions=3, in_channels=1, out_channels=1, channels=(16, 32, 64, 128, 256),
               strides=(2, 2, 2, 2), num_res_units=2, norm=Norm.BATCH, dropout=0).to(device)
    loss = GeneralizedDiceLoss(sigmoid=True)
    opt = torch.optim.Adam(net.parameters(), 1e-3)

    # trainer = create_supervised_trainer(net, opt, loss, device, False, )
    trainer = SupervisedTrainer(
        device=device,
        max_epochs=1000,
        train_data_loader=loader,
        network=net,
        optimizer=opt,
        loss_function=loss,
        key_train_metric={"train_meandice": MeanDice(sigmoid=True,output_transform=lambda x: (x["pred"], x["label"]))},
    )
	
	lr_scheduler = StepLR(opt, 400)
	
	
Run 27 - Val mean dice 0.823 at epoch 580 (0.817 at 1200)
	xform = Compose([
			LoadNiftid(keys),
			AddChanneld(keys),
			Spacingd(keys, 0.5, diagonal=True, mode=('bilinear', 'nearest')),
			Orientationd(keys, axcodes='RAS'),
			ScaleIntensityd("image"),
			CropForegroundd(keys, source_key="image"),
			RandCropByPosNegLabeld(keys, label_key='label', spatial_size=(96,96,96), pos=0.8, neg=0.2, num_samples=4),
			ToTensord(keys)
		])
	loader = DataLoader(ds, batch_size=7, shuffle=True, num_workers=0, drop_last=True)
    net = UNet(dimensions=3, in_channels=1, out_channels=1, channels=(16, 32, 64, 128, 256),
               strides=(2, 2, 2, 2), num_res_units=2, norm=Norm.BATCH, dropout=0.05).to(device)
    loss = GeneralizedDiceLoss(sigmoid=True)
    opt = torch.optim.Adam(net.parameters(), 1e-3)

    # trainer = create_supervised_trainer(net, opt, loss, device, False, )
    trainer = SupervisedTrainer(
        device=device,
        max_epochs=1000,
        train_data_loader=loader,
        network=net,
        optimizer=opt,
        loss_function=loss,
        key_train_metric={"train_meandice": MeanDice(sigmoid=True,output_transform=lambda x: (x["pred"], x["label"]))},
    )
	
	lr_scheduler = StepLR(opt, 600)
	
	Notes; Possibly best so far for generalizability, best Hausdorff error on all validation cases.
	
		
Run 28 - Val mean dice 0.819 at epoch 770 (0.814 at 1200)
	xform = Compose([
			LoadNiftid(keys),
			AddChanneld(keys),
			Spacingd(keys, 0.5, diagonal=True, mode=('bilinear', 'nearest')),
			Orientationd(keys, axcodes='RAS'),
			ScaleIntensityd("image"),
			CropForegroundd(keys, source_key="image"),
			RandCropByPosNegLabeld(keys, label_key='label', spatial_size=(96,96,96), pos=0.8, neg=0.2, num_samples=4),
			ToTensord(keys)
		])
	loader = DataLoader(ds, batch_size=7, shuffle=True, num_workers=0, drop_last=True)
    net = UNet(dimensions=3, in_channels=1, out_channels=1, channels=(16, 32, 64, 128, 256),
               strides=(2, 2, 2, 2), num_res_units=2, norm=Norm.BATCH, dropout=0.05).to(device)
    loss = TverskyLoss(sigmoid=True, alpha=0.9, beta=0.1)
    opt = torch.optim.Adam(net.parameters(), 1e-3)

    # trainer = create_supervised_trainer(net, opt, loss, device, False, )
    trainer = SupervisedTrainer(
        device=device,
        max_epochs=1200,
        train_data_loader=loader,
        network=net,
        optimizer=opt,
        loss_function=loss,
        key_train_metric={"train_meandice": MeanDice(sigmoid=True,output_transform=lambda x: (x["pred"], x["label"]))},
    )
	
	lr_scheduler = StepLR(opt, 600)
	
	Notes; TVersky loss not significantly different than generalizeddice
	
Run 29 - Val mean dice 0.819 at epoch 770 (0.814 at 1200)
	xform = Compose([
        LoadNiftid(keys),
        AddChanneld(keys),
        Spacingd(keys, 0.5, diagonal=True, mode=('bilinear', 'nearest')),
        Orientationd(keys, axcodes='RAS'),
        ScaleIntensityd("image"),
        CropForegroundd(keys, source_key="image"),
        #RandAffined(keys, mode=('bilinear', 'nearest'), rotate_range=(0.15,0.15,0.15), scale_range=(0.05,0.05,0.05), prob=0.2, as_tensor_output=False, device=device),
        # Rand3DElasticd(keys, mode=('bilinear', 'nearest'),  rotate_range=(0.15,0.15,0.15), scale_range=(0.05,0.05,0.05),
        #                sigma_range=(0,1), magnitude_range=(0,2), prob=0.2, as_tensor_output=False, device=device),
        RandCropByPosNegLabeld(keys, label_key='label', spatial_size=(96,96,96), pos=0.8, neg=0.2, num_samples=4),
        ToTensord(keys)
    ])

    loader = DataLoader(ds, batch_size=8, shuffle=True, num_workers=0, drop_last=True, pin_memory=torch.cuda.is_available())
	
	net = UNet(dimensions=3, in_channels=1, out_channels=1, channels=(16, 32, 64, 128, 256),
               strides=(2, 2, 2, 2), num_res_units=2, norm=Norm.BATCH, dropout=0.05).to(device)
    loss = GeneralizedDiceLoss(sigmoid=True)
    opt = Novograd(net.parameters(), 1e-3)
	trainer = SupervisedTrainer(
        device=device,
        max_epochs=4000,
        train_data_loader=loader,
        network=net,
        optimizer=opt,
        loss_function=loss,
        key_train_metric={"train_meandice": MeanDice(sigmoid=True,output_transform=lambda x: (x["pred"], x["label"]))},
        amp=True
    )
	lr_scheduler = StepLR(opt, 600)
	
	Notes: Novograd scheduler and slightly increased batch size from using amp -> mild improvement.

Run 31 - Val mean dice 0.816 at epoch 1270 (0.802 at 3660)
	xform = Compose([
        LoadNiftid(keys),
        AddChanneld(keys),
        Spacingd(keys, 0.5, diagonal=True, mode=('bilinear', 'nearest')),
        Orientationd(keys, axcodes='RAS'),
        ScaleIntensityd("image"),
        CropForegroundd(keys, source_key="image"),
        #RandAffined(keys, mode=('bilinear', 'nearest'), rotate_range=(0.15,0.15,0.15), scale_range=(0.05,0.05,0.05), prob=0.2, as_tensor_output=False, device=device),
        # Rand3DElasticd(keys, mode=('bilinear', 'nearest'),  rotate_range=(0.15,0.15,0.15), scale_range=(0.05,0.05,0.05),
        #                sigma_range=(0,1), magnitude_range=(0,2), prob=0.2, as_tensor_output=False, device=device),
        RandCropByPosNegLabeld(keys, label_key='label', spatial_size=(96,96,96), pos=0.8, neg=0.2, num_samples=4),
        ToTensord(keys)
    ])

    loader = DataLoader(ds, batch_size=8, shuffle=True, num_workers=0, drop_last=True, pin_memory=torch.cuda.is_available())
	
	net = UNet(dimensions=3, in_channels=1, out_channels=1, channels=(16, 32, 64, 128, 256),
               strides=(2, 2, 2, 2), num_res_units=2, norm=Norm.BATCH, dropout=0.05).to(device)
    loss = GeneralizedDiceLoss(sigmoid=True)
    opt = Novograd(net.parameters(), 1e-3)
	trainer = SupervisedTrainer(
        device=device,
        max_epochs=4000,
        train_data_loader=loader,
        network=net,
        optimizer=opt,
        loss_function=loss,
        key_train_metric={"train_meandice": MeanDice(sigmoid=True,output_transform=lambda x: (x["pred"], x["label"]))},
        amp=True
    )
	
	Notes: HD 8.25, ASD 1.341 voxels
	
Run 32 - Val mean dice 0.820 at epoch 470 (0.807 at 3000)
	xform = Compose([
        LoadNiftid(keys),
        AddChanneld(keys),
        Spacingd(keys, 0.5, diagonal=True, mode=('bilinear', 'nearest')),
        Orientationd(keys, axcodes='RAS'),
        ScaleIntensityd("image"),
        CropForegroundd(keys, source_key="image"),
        #RandAffined(keys, mode=('bilinear', 'nearest'), rotate_range=(0.15,0.15,0.15), scale_range=(0.05,0.05,0.05), prob=0.2, as_tensor_output=False, device=device),
        # Rand3DElasticd(keys, mode=('bilinear', 'nearest'),  rotate_range=(0.15,0.15,0.15), scale_range=(0.05,0.05,0.05),
        #                sigma_range=(0,1), magnitude_range=(0,2), prob=0.2, as_tensor_output=False, device=device),
        RandCropByPosNegLabeld(keys, label_key='label', spatial_size=(96,96,96), pos=0.8, neg=0.2, num_samples=4),
        ToTensord(keys)
    ])

    loader = DataLoader(ds, batch_size=8, shuffle=True, num_workers=0, drop_last=True, pin_memory=torch.cuda.is_available())
	
	net = UNet(dimensions=3, in_channels=1, out_channels=1, channels=(16, 32, 64, 128, 256),
               strides=(2, 2, 2, 2), num_res_units=2, norm=Norm.BATCH, dropout=0.0).to(device)
    loss = GeneralizedDiceLoss(sigmoid=True)
    opt = Novograd(net.parameters(), 1e-2)
	trainer = SupervisedTrainer(
        device=device,
        max_epochs=3000,
        train_data_loader=loader,
        network=net,
        optimizer=opt,
        loss_function=loss,
        key_train_metric={"train_meandice": MeanDice(sigmoid=True,output_transform=lambda x: (x["pred"], x["label"]))},
        amp=True
    )
	lr_scheduler = StepLR(opt, 1000)
	
Run 33 - Val mean dice 0.805 at epoch 970 ( 0.777 at 3000)
	xform = Compose([
        LoadNiftid(keys),
        AddChanneld(keys),
        Spacingd(keys, 0.5, diagonal=True, mode=('bilinear', 'nearest')),
        Orientationd(keys, axcodes='RAS'),
        ScaleIntensityd("image"),
        CropForegroundd(keys, source_key="image"),
        #RandAffined(keys, mode=('bilinear', 'nearest'), rotate_range=(0.15,0.15,0.15), scale_range=(0.05,0.05,0.05), prob=0.2, as_tensor_output=False, device=device),
        # Rand3DElasticd(keys, mode=('bilinear', 'nearest'),  rotate_range=(0.15,0.15,0.15), scale_range=(0.05,0.05,0.05),
        #                sigma_range=(0,1), magnitude_range=(0,2), prob=0.2, as_tensor_output=False, device=device),
        RandCropByPosNegLabeld(keys, label_key='label', spatial_size=(96,96,96), pos=0.8, neg=0.2, num_samples=4),
        ToTensord(keys)
    ])

    loader = DataLoader(ds, batch_size=8, shuffle=True, num_workers=0, drop_last=True, pin_memory=torch.cuda.is_available())
	
	net = UNet(dimensions=3, in_channels=1, out_channels=1, channels=(16, 32, 64, 128, 256),
               strides=(2, 2, 2, 2), num_res_units=2, norm=Norm.BATCH, dropout=0.0).to(device)
			   
	gd = GeneralizedDiceLoss(sigmoid=True)
    bce = torch.nn.BCEWithLogitsLoss()

    def loss(input,target):
        return gd(input,target) + (1 - 1 / torch.exp(bce(input,target)))
			   
    opt = Novograd(net.parameters(), 1e-3)
	trainer = SupervisedTrainer(
        device=device,
        max_epochs=3000,
        train_data_loader=loader,
        network=net,
        optimizer=opt,
        loss_function=loss,
        key_train_metric={"train_meandice": MeanDice(sigmoid=True,output_transform=lambda x: (x["pred"], x["label"]))},
        amp=True
    )
	lr_scheduler = StepLR(opt, 1500)
	
	Notes: Metrics -- AvgSurfaceDistance: 1.5766 HausdorffDistance: 7.5878
	
Run 34 - Val mean dice 0.825 at epoch 790 ( 0.8045 at 2000)
	xform = Compose([
        LoadNiftid(keys),
        AddChanneld(keys),
        Spacingd(keys, 0.5, diagonal=True, mode=('bilinear', 'nearest')),
        Orientationd(keys, axcodes='RAS'),
        ScaleIntensityd("image"),
        CropForegroundd(keys, source_key="image"),
        #RandAffined(keys, mode=('bilinear', 'nearest'), rotate_range=(0.15,0.15,0.15), scale_range=(0.05,0.05,0.05), prob=0.2, as_tensor_output=False, device=device),
        # Rand3DElasticd(keys, mode=('bilinear', 'nearest'),  rotate_range=(0.15,0.15,0.15), scale_range=(0.05,0.05,0.05),
        #                sigma_range=(0,1), magnitude_range=(0,2), prob=0.2, as_tensor_output=False, device=device),
        RandCropByPosNegLabeld(keys, label_key='label', spatial_size=(96,96,96), pos=0.8, neg=0.2, num_samples=4),
        ToTensord(keys)
    ])

    loader = DataLoader(ds, batch_size=8, shuffle=True, num_workers=0, drop_last=True, pin_memory=torch.cuda.is_available())
	
	net = UNet(dimensions=3, in_channels=1, out_channels=1, channels=(16, 32, 64, 128, 256),
               strides=(2, 2, 2, 2), num_res_units=2, norm=Norm.BATCH, dropout=0.0).to(device)
			   
	gd = GeneralizedDiceLoss(sigmoid=True)
    bce = torch.nn.BCEWithLogitsLoss()

    def loss(input,target):
        return gd(input,target) + bce(input,target)
			   
    opt = Novograd(net.parameters(), 1e-2)
	trainer = SupervisedTrainer(
        device=device,
        max_epochs=2000,
        train_data_loader=loader,
        network=net,
        optimizer=opt,
        loss_function=loss,
        key_train_metric={"train_meandice": MeanDice(sigmoid=True,output_transform=lambda x: (x["pred"], x["label"]))},
        amp=True
    )
	lr_scheduler = StepLR(opt, 1000)
	
	Notes: Metrics -- AvgSurfaceDistance: 1.0002 HausdorffDistance: 7.0929
	
	
Run 35 - Val mean dice _ at epoch _ ( _ at _)
	xform = Compose([
        LoadNiftid(keys),
        AddChanneld(keys),
        Spacingd(keys, 0.5, diagonal=True, mode=('bilinear', 'nearest')),
        Orientationd(keys, axcodes='RAS'),
        ScaleIntensityd("image"),
        CropForegroundd(keys, source_key="image"),
        #RandAffined(keys, mode=('bilinear', 'nearest'), rotate_range=(0.15,0.15,0.15), scale_range=(0.05,0.05,0.05), prob=0.2, as_tensor_output=False, device=device),
        # Rand3DElasticd(keys, mode=('bilinear', 'nearest'),  rotate_range=(0.15,0.15,0.15), scale_range=(0.05,0.05,0.05),
        #                sigma_range=(0,1), magnitude_range=(0,2), prob=0.2, as_tensor_output=False, device=device),
        RandCropByPosNegLabeld(keys, label_key='label', spatial_size=(96,96,96), pos=0.8, neg=0.2, num_samples=4),
        ToTensord(keys)
    ])

    loader = DataLoader(ds, batch_size=1, shuffle=True, num_workers=0, drop_last=True, pin_memory=torch.cuda.is_available())
	
	net = VNet(spatial_dims=3, in_channels=1, out_channels=1).to(device)
			   
	gd = GeneralizedDiceLoss(sigmoid=True)
    bce = torch.nn.BCEWithLogitsLoss()

    def loss(input,target):
        return gd(input,target) + bce(input,target)
			   
    opt = Novograd(net.parameters(), 1e-2)
	trainer = SupervisedTrainer(
        device=device,
        max_epochs=2000,
        train_data_loader=loader,
        network=net,
        optimizer=opt,
        loss_function=loss,
        key_train_metric={"train_meandice": MeanDice(sigmoid=True,output_transform=lambda x: (x["pred"], x["label"]))},
        amp=True
    )
	lr_scheduler = StepLR(opt, 1000)
	
	Notes: Metrics -- AvgSurfaceDistance: 1.044 HausdorffDistance: 8.18, slow training (nearly 2 days)
	

Run 36
	xform = Compose([
        LoadNiftid(keys),
        AddChanneld(keys),
        Spacingd(keys, (0.5,0.5,0.25), diagonal=True, mode=('bilinear', 'nearest')),
        Orientationd(keys, axcodes='RAS'),
        ScaleIntensityd("image"),
        CropForegroundd(keys, source_key="image"),
        #RandAffined(keys, mode=('bilinear', 'nearest'), rotate_range=(0.15,0.15,0.15), scale_range=(0.05,0.05,0.05), prob=0.2, as_tensor_output=False, device=device),
        # Rand3DElasticd(keys, mode=('bilinear', 'nearest'),  rotate_range=(0.15,0.15,0.15), scale_range=(0.05,0.05,0.05),
        #                sigma_range=(0,1), magnitude_range=(0,2), prob=0.2, as_tensor_output=False, device=device),
        RandCropByPosNegLabeld(keys, label_key='label', spatial_size=(128,128,96), pos=0.8, neg=0.2, num_samples=2),
        ToTensord(keys)
    ])
	
	loader = DataLoader(ds, batch_size=1, shuffle=True, num_workers=0, drop_last=True, pin_memory=torch.cuda.is_available())
	
	net = AHNet().to(device)
	
	gd = GeneralizedDiceLoss(sigmoid=True)
    bce = torch.nn.BCEWithLogitsLoss()

    def loss(input,target):
        return gd(input,target) + bce(input,target)
			   
    opt = Novograd(net.parameters(), 1e-2)
	
	trainer = SupervisedTrainer(
        device=device,
        max_epochs=2000,
        train_data_loader=loader,
        network=net,
        optimizer=opt,
        loss_function=loss,
        key_train_metric={"train_meandice": MeanDice(sigmoid=True,output_transform=lambda x: (x["pred"], x["label"]))},
        amp=True
    )
	
	lr_scheduler = StepLR(opt, 1000)
	
	Note: Diverges at 800 epochs, poor performance, slow training (23hrs)
	
Run 37
	xform = Compose([
        LoadNiftid(keys),
        AddChanneld(keys),
        Spacingd(keys, (0.5,0.5,0.3), diagonal=True, mode=('bilinear', 'nearest')),
        Orientationd(keys, axcodes='RAS'),
        ScaleIntensityd("image"),
        CropForegroundd(keys, source_key="image"),
        #RandAffined(keys, mode=('bilinear', 'nearest'), rotate_range=(0.15,0.15,0.15), scale_range=(0.05,0.05,0.05), prob=0.2, as_tensor_output=False, device=device),
        # Rand3DElasticd(keys, mode=('bilinear', 'nearest'),  rotate_range=(0.15,0.15,0.15), scale_range=(0.05,0.05,0.05),
        #                sigma_range=(0,1), magnitude_range=(0,2), prob=0.2, as_tensor_output=False, device=device),
        RandCropByPosNegLabeld(keys, label_key='label', spatial_size=(64,64,64), pos=0.8, neg=0.2, num_samples=4),
        ToTensord(keys)
    ])
	
	loader = DataLoader(ds, batch_size=4, shuffle=True, num_workers=0, drop_last=True, pin_memory=torch.cuda.is_available())
	
	net = SegResNetVAE(input_image_size=(64,64,64), out_channels=1, norm_name='batch').to(device)
	
	gd = GeneralizedDiceLoss(sigmoid=True)
    bce = torch.nn.BCEWithLogitsLoss()

    def loss(input,target):
        return gd(input[0],target) + bce(input[0],target)

    opt = Novograd(net.parameters(), 1e-2)
	
	trainer = SupervisedTrainer(
        device=device,
        max_epochs=2000,
        train_data_loader=loader,
        network=net,
        optimizer=opt,
        loss_function=loss,
        key_train_metric={"train_meandice": MeanDice(sigmoid=True,output_transform=lambda x: (x["pred"][0], x["label"]))},
        amp=True
    )
	
	lr_scheduler = StepLR(opt, 1000)
	
	Notes: Bathtub curve with validation metrics getting worse after 1300 epochs
	


Things to test: LR Scheduling (step, plateau)
	
	
	
